# Scalability: Distributed and Parallelized Programming
## Table of Contents
- Context
  - Big Data
  - High Performance Computing (HPC)
- The Need for Scalability
  - Scale up: vertical/hardware-based scalability
  - Scale out: horizontal/software-based scalability
- Parallelization and Cluster Computing
  - A motivating example
  - Map and Reduce parallelized operations
- [Spark](https://spark.apache.org/docs/latest/index.html)
  - Services
    - Resource management
    - Distributed processing
      - Resilient Distributed Datasets (RDDs)
      - Operations
        - Transformations
        - Actions
  - [Architecture](https://spark.apache.org/docs/latest/cluster-overview.html)
    - Cluster manager
      - [Spark Standalone](https://spark.apache.org/docs/latest/spark-standalone.html)
      - [YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)
      - [Kubernetes](https://kubernetes.io)
    - Driver program
      - SparkContext
    - Worker nodes
    - Executors
    - Tasks
  - [Case Study: Ecommerce Product Rating](./case-studies/rating)
  - Installation (Spark Standalone)
  - Deployment (Manual)
    - ./sbin/start-master.sh
    - ./sbin/start-worker.sh
  - Application Deployment
    - [./bin/spark-submit.sh](https://spark.apache.org/docs/latest/submitting-applications.html)
- [Kafka](https://kafka.apache.org/)
  - [Architecture](./figures/Kafka-Cluster.png)
    - Server / Broker
    - Cluster
    - Topics and Partitions
    - Producers
    - Consumers (Batch mode vs. Stream mode)
    - [Consumer Groups](./figures/Kafka-Cluster-Consumer-Group.png)
