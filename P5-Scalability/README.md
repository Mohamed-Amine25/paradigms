# Scalability: Distributed and Parallelized Programming
## Table of Contents
- Context
  - Big Data
  - High Performance Computing (HPC)
- The Need for Scalability
  - Scale up: vertical/hardware-based scalability
  - Scale out: horizontal/software-based scalability
- Parallelization and Cluster Computing
  - A motivating example
  - Map and Reduce parallelized operations
- [Spark](https://spark.apache.org/docs/latest/index.html)
  - Services
    - Resource management
    - Distributed processing
      - Resilient Distributed Datasets (RDDs)
      - Operations
        - Transformations
        - Actions
  - [Architecture](https://spark.apache.org/docs/latest/cluster-overview.html)
  <p align="center"><img src="./figures/Spark-Architecture.png"></p>
    - Cluster manager
      - [Spark Standalone](https://spark.apache.org/docs/latest/spark-standalone.html)
      - [Mesos](http://mesos.apache.org)
      - [YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)
      - [Kubernetes](https://kubernetes.io)
    - Driver program
      - SparkContext
    - Worker nodes
    - Executors
    - Tasks
  - Installation (Spark Standalone)
  - Deployment (Manual)
    - ./sbin/start-master.sh
    - ./sbin/start-worker.sh
  - Application Deployment
    - [./bin/spark-submit.sh](https://spark.apache.org/docs/latest/submitting-applications.html)
- [Case Study: Ecommerce Product Rating](./case-studies/rating)
